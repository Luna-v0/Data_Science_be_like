When Agents learn the practical problem is that the number of states and actions can grow indefinitely depending on the problem, and depending on the algorithm used, it might need more samples of the environment to learn, so the problem becomes, use a action already used or explore unknowns. 
## Heuristics
To try to mitigate this problem and bring some kind of balance there are several heuristics for saying if a agent should explore or should exploit. All of them can have some kind of decay, meaning it some how it can start decreasing the amount of exploration the agent does, this decay can be implemented in multiple forms like in each iteration decrease a value, or multiple by a value between 0 and 1 for faster decay. 
### $\epsilon$ greedy
This is the most famous of them all. Which says that Agents should choose a random policy with probability $\epsilon$ and should choose the best action with probability $1-\epsilon$.
### Boltzmann
Based on the principal of [[Simulated Annealing]] it follows the Boltzmann equation of probability, and creates a probability for each state-action pair.
$$
P(s,a) =  \frac{e^{Q(s,a) / T}}{\sum_{b \in A} e^{Q(s,b) / T}} 
$$
It obviously takes more time to compute then $\epsilon$ Greedy.

