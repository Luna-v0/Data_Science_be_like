{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c7a568",
   "metadata": {},
   "source": [
    "# RAG from Scracth\n",
    "\n",
    "This notebook goal is to implement a simple [[Retrieval Augmented Generation (RAG)]] using [[Vector#Dot Product|Cosine Simillarity]].\n",
    "\n",
    "The pipelines goes like this: first you create [[Retrieval Augmented Generation (RAG)#Chunks|Chunks]] for the documents in which you want to search and create [[Embeddings]]. When the user asks a questions, it creates embeddings of the questions and using a similarity score retrieve the top $K$ results, and feed them to the [[Language Models|LM]].\n",
    "\n",
    "# Requirements\n",
    "\n",
    "You must have ollama installed. Use this to get the same model as I am.\n",
    "\n",
    "```bash\n",
    "ollama run gemma3:4b\n",
    "```\n",
    "\n",
    "You also must have authorization from this link: \n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25a85165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 17:53:10.788819: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-26 17:53:10.796219: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769460790.804473   59637 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769460790.807452   59637 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769460790.815022   59637 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769460790.815029   59637 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769460790.815030   59637 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769460790.815031   59637 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-26 17:53:10.817630: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "EMBEDDING_MODEL = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "LM_VERSION = \"gemma3:4b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f1f83",
   "metadata": {},
   "source": [
    "For this notebook I will use this obsidian vault as the dataset. I will try to use each separated file as a chunk since there isn't no really big files.\n",
    "\n",
    "For context of the reader, EE is Exploration vs Exploitation in my notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcea6346",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"give me two types of EE heuristics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8023f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_DB = []\n",
    "\n",
    "def add_chunk_to_database(chunk):\n",
    "  embedding = EMBEDDING_MODEL.encode(chunk)\n",
    "  VECTOR_DB.append((chunk, embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c718eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../notes/01-concepts/linear-algebra/Quadratic Form.md',\n",
       " '../../notes/01-concepts/linear-algebra/Vector Spaces.md',\n",
       " '../../notes/01-concepts/linear-algebra/Kernel and Image.md',\n",
       " '../../notes/01-concepts/linear-algebra/Linear Systems.md',\n",
       " '../../notes/01-concepts/linear-algebra/Matrix.md',\n",
       " '../../notes/01-concepts/linear-algebra/Linearity.md',\n",
       " '../../notes/01-concepts/linear-algebra/Orthonormal and Orthogonal.md',\n",
       " '../../notes/01-concepts/linear-algebra/Types of Matrices.md',\n",
       " '../../notes/01-concepts/linear-algebra/Base vectors.md',\n",
       " '../../notes/01-concepts/linear-algebra/Unit Vector.md',\n",
       " '../../notes/01-concepts/linear-algebra/Internal Product.md',\n",
       " '../../notes/01-concepts/linear-algebra/Vector.md',\n",
       " '../../notes/01-concepts/linear-algebra/Base change.md',\n",
       " '../../notes/01-concepts/linear-algebra/Projection.md',\n",
       " '../../notes/01-concepts/linear-algebra/Tensor.md',\n",
       " '../../notes/01-concepts/linear-algebra/Linear definitions.md',\n",
       " '../../notes/01-concepts/linear-algebra/Eigenvalues and Eigenvectors.md',\n",
       " '../../notes/01-concepts/finance/Compound Interest.md',\n",
       " '../../notes/01-concepts/classic-ai/Simulated Annealing.md',\n",
       " '../../notes/01-concepts/classic-ai/Min-Max.md',\n",
       " '../../notes/01-concepts/calculus/Hessian.md',\n",
       " '../../notes/01-concepts/calculus/Vector fields.md',\n",
       " '../../notes/01-concepts/calculus/Derivatives.md',\n",
       " '../../notes/01-concepts/calculus/Partial Derivatives.md',\n",
       " '../../notes/01-concepts/calculus/Integral.md',\n",
       " '../../notes/01-concepts/calculus/Jacobian.md',\n",
       " '../../notes/01-concepts/machine-learning/Neural Networks (NN).md',\n",
       " '../../notes/01-concepts/machine-learning/Gradient Decent.md',\n",
       " '../../notes/01-concepts/machine-learning/Model Learning.md',\n",
       " '../../notes/01-concepts/machine-learning/Hyperparameter Tuning.md',\n",
       " '../../notes/01-concepts/machine-learning/Attention.md',\n",
       " '../../notes/01-concepts/machine-learning/Graph Neural Network (GNN).md',\n",
       " '../../notes/01-concepts/machine-learning/Perceptron.md',\n",
       " '../../notes/01-concepts/machine-learning/Transformers.md',\n",
       " '../../notes/01-concepts/machine-learning/Convolution Neural Networks (CNN).md',\n",
       " '../../notes/01-concepts/machine-learning/Recurrent Neural Network (RNN).md',\n",
       " '../../notes/01-concepts/machine-learning/Embeddings.md',\n",
       " '../../notes/01-concepts/machine-learning/Supervised/Support Vector Machine.md',\n",
       " '../../notes/01-concepts/machine-learning/Supervised/Linear Regression.md',\n",
       " '../../notes/01-concepts/machine-learning/Supervised/Classification vs Regression.md',\n",
       " '../../notes/01-concepts/machine-learning/Reinforcement/POMDP.md',\n",
       " '../../notes/01-concepts/machine-learning/Reinforcement/Temporal Difference (TD) Learning.md',\n",
       " '../../notes/01-concepts/machine-learning/Reinforcement/Iteration Algorithms (Planning).md',\n",
       " '../../notes/01-concepts/machine-learning/Reinforcement/DQN.md',\n",
       " '../../notes/01-concepts/machine-learning/Reinforcement/Q-Learning.md',\n",
       " '../../notes/01-concepts/machine-learning/Reinforcement/Actor Critic.md',\n",
       " '../../notes/01-concepts/machine-learning/Reinforcement/Markov Decision Process (MDP).md',\n",
       " '../../notes/01-concepts/machine-learning/Reinforcement/Policy Gradient.md',\n",
       " '../../notes/01-concepts/machine-learning/Reinforcement/Sarsa.md',\n",
       " '../../notes/01-concepts/machine-learning/Reinforcement/Exploration vs Exploitation.md',\n",
       " '../../notes/01-concepts/machine-learning/Natural Language/RAG.md',\n",
       " '../../notes/01-concepts/machine-learning/Natural Language/Language Models.md',\n",
       " '../../notes/01-concepts/machine-learning/Natural Language/Prompt Engineering.md',\n",
       " '../../notes/01-concepts/machine-learning/Natural Language/LLM Agents.md',\n",
       " '../../notes/01-concepts/machine-learning/Natural Language/LLM & Thought.md',\n",
       " '../../notes/01-concepts/machine-learning/Natural Language/Tokenizers.md',\n",
       " '../../notes/01-concepts/machine-learning/Unsupervised/Principal Component Analyses.md',\n",
       " '../../notes/01-concepts/statistics/Covariance.md',\n",
       " '../../notes/01-concepts/statistics/Covariance Matrix.md',\n",
       " '../../notes/01-concepts/statistics/Variance.md',\n",
       " '../../notes/01-concepts/statistics/Pearson Correlation.md',\n",
       " '../../notes/01-concepts/statistics/Expected Value.md',\n",
       " '../../notes/02-literature/Untitled.md']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = glob(\"../../notes/**/**.md\", recursive=True)\n",
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d30ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load each file and add it to the database\n",
    "for file in filepath:\n",
    "  with open(file, \"r\") as f:\n",
    "    content = f.read()\n",
    "    # append the file name to the chunk\n",
    "    if content == \"\": continue\n",
    "    content = f\"File: {file}\\n{content}\"\n",
    "    \n",
    "    add_chunk_to_database(content)\n",
    "\n",
    "# turn the database into a numpy array\n",
    "VECTOR_DB_NP = np.array([v[1] for v in VECTOR_DB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c2eacb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['File: ../../notes/01-concepts/machine-learning/Reinforcement/Exploration vs Exploitation.md\\nWhen Agents learn the practical problem is that the number of states and actions can grow indefinitely depending on the problem, and depending on the algorithm used, it might need more samples of the environment to learn, so the problem becomes, use a action already used or explore unknowns. \\n## Heuristics\\nTo try to mitigate this problem and bring some kind of balance there are several heuristics for saying if a agent should explore or should exploit. All of them can have some kind of decay, meaning it some how it can start decreasing the amount of exploration the agent does, this decay can be implemented in multiple forms like in each iteration decrease a value, or multiple by a value between 0 and 1 for faster decay. \\n### $\\\\epsilon$ greedy\\nThis is the most famous of them all. Which says that Agents should choose a random policy with probability $\\\\epsilon$ and should choose the best action with probability $1-\\\\epsilon$.\\n### Boltzmann\\nBased on the principal of [[Simulated Annealing]] it follows the Boltzmann equation of probability, and creates a probability for each state-action pair.\\n$$\\nP(s,a) =  \\\\frac{e^{Q(s,a) / T}}{\\\\sum_{b \\\\in A} e^{Q(s,b) / T}} \\n$$\\nIt obviously takes more time to compute then $\\\\epsilon$ Greedy.\\n\\n', 'File: ../../notes/01-concepts/machine-learning/Natural Language/Prompt Engineering.md\\nAre a set of techniques for creating better [[Language Models#Prompts|prompts]]. The main topic on it is context, there are many problems involved context, like missing information, bad training data, hallucination, ambiguity, etc.\\n\\n## Guidelines\\nThose are 5 main guidelines for creating better prompts.\\n\\n1. **Clarity:** It must be exact about what you want.\\n2. **Context**: More information about the subject helps the [[Language Models|LM]] to respond better, but they must be relevant, sufficient and not large as it can be to avoid [[Language Models#Prompts|Context Window]] problems. \\n3. **Objective:** Be concise and avoid ambiguity.\\n4. **Detail:** Detail makes guides better the response.\\n5. **Purpose:** What is the objective. \\n\\nOther useful stuff is to deal with [[Model Learning#Hyperparameters|Hyperparameters]] for instance lower temperature tend to avoid hallucination.\\n\\n\\n## Roles \\n\\n\\n\\n## Few, one and zero Shot\\n\"Shot\" in this sense is examples of response. \\n\\n## Iterative Prompting\\nImproving by testing the prompt. Its the most common and simple technique of prompt engineering that can be used together with other techniques. \\n\\n## Chain of thought (CoT) prompt\\nIs a induction technique to induce the response of the [[Language Models]] linearly, some of the most common:\\n* Asking for solving step by step\\n* Ask it to argument its response\\n* Giving fixed options for its response\\n\\n**Example:**\\n```\\nProblem: Sarah has 15 apples. She gives 1/3 of them to her friend and then buys 8 more. How many apples does she have now?\\n\\nSolve this step by step:\\n1. First, calculate how many apples Sarah gave away\\n2. Then, calculate how many apples she has left\\n3. Finally, add the new apples she bought\\n4. Provide the final answer\\n```\\n## Tree of thought (ToT) prompt\\nIt\\'s a hierarchic technique for prompting, this is some template for it:\\n\\n**Example:**\\n```\\nTask: Plan a weekend trip to a new city with a $500 budget.\\n\\nExplore different approaches:\\n\\nBranch 1: Focus on accommodation\\n- Option A: Luxury hotel ($300) → limited budget for activities\\n- Option B: Budget hostel ($80) → more budget for experiences\\n- Option C: Airbnb ($150) → balanced approach\\n\\nBranch 2: Focus on transportation\\n- Option A: Rent a car ($200) → freedom but high cost\\n- Option B: Public transport ($30) → economical but limited\\n- Option C: Bike rental ($50) → eco-friendly and flexible\\n\\nEvaluate each branch and select the best combination based on:\\n1. Total cost\\n2. Experience quality\\n3. Flexibility\\n\\nProvide the optimal plan.\\n```\\n\\n## Graph of thought (GoT)\\nExtends ToT by allowing thoughts to connect in a non-hierarchical graph structure, enabling cycles and multiple connections between concepts.\\n\\n**Example:**\\n```\\nTask: Debug why a web application is running slowly.\\n\\nCreate a graph of interconnected potential causes:\\n\\nNode A: Slow database queries\\n  → connects to Node C (inefficient indexes)\\n  → connects to Node E (too many connections)\\n\\nNode B: Large bundle size\\n  → connects to Node D (unoptimized images)\\n  → connects to Node F (unused dependencies)\\n\\nNode C: Inefficient indexes\\n  → connects back to Node A (affects query speed)\\n  → connects to Node G (poor database schema)\\n\\nNode D: Unoptimized images\\n  → connects to Node B (increases bundle)\\n  → connects to Node H (slow CDN delivery)\\n\\nNode E: Too many connections\\n  → connects to Node A (database bottleneck)\\n  → connects to Node I (connection pooling issues)\\n\\nAnalyze the graph to identify:\\n1. Most connected nodes (likely root causes)\\n2. Cycles that compound the problem\\n3. Priority order for fixes\\n```\\n\\n## Prompt Agentic\\nIts based on an autonomous agent approach for the LM, its principal are to plan, decide, execute action, evaluate the result and iterate over it. The prompt must define behavior of the agent, responsibility, limits, decision criteria, a \"reasoning\" and \"action\" loop.\\n\\nThis is one of the best resulting prompt techniques.\\n\\n**Example:**\\n```\\nYou are a research agent tasked with analyzing market trends for electric vehicles.\\n\\nBehavior: You are analytical, data-driven, and thorough.\\nResponsibility: Gather data, identify trends, and provide actionable insights.\\nLimits: Use only publicly available data from 2020-2025. Do not speculate beyond available data.\\n\\nDecision Criteria:\\n- Prioritize data from reputable sources (government reports, industry publications)\\n- Cross-reference multiple sources for validation\\n- Focus on trends with statistical significance\\n\\nReasoning and Action Loop:\\n1. PLAN: Identify what information is needed\\n2. ACT: Describe what data sources you would access\\n3. OBSERVE: Summarize the findings from each source\\n4. REASON: Analyze patterns and connections\\n5. DECIDE: Determine if more information is needed or if conclusions can be drawn\\n6. ITERATE: If needed, return to step 1 with refined focus\\n\\nTask: Analyze the adoption rate of electric vehicles in Europe from 2020-2025.\\nBegin with your plan.\\n``` ', 'File: ../../notes/01-concepts/machine-learning/Unsupervised/Principal Component Analyses.md\\n#ML \\n\\n', 'File: ../../notes/01-concepts/machine-learning/Reinforcement/Sarsa.md\\nIs a [[Markov Decision Process (MDP)#Learning a MDP|on policy]] algorithm based on [[Temporal Difference (TD) Learning|TD Control]], it uses the following variation:\\n\\n$$\\n\\\\begin{gather}\\n\\\\delta_t = r_{t+1} + \\\\gamma \\\\; Q(s_{t+1},a_{t+1}) - Q(s_t,a_t) \\\\\\\\ \\\\\\\\ \\nQ(s_t,a_t) = Q(s_t,a_t) + \\\\alpha \\\\; \\\\delta_t\\n\\\\end{gather}\\n$$\\n\\nThe overall algorithm does as following:\\n1. Start with a zeroed Q matrix.\\n2. Initialize current state and action using a [[Exploration vs Exploitation#Heuristics|EE heuristics]].\\n3. For each step:\\n\\t1. Take a observation  $a_t \\\\to (s_t,r_t)$\\n\\t2. Choose some action (using the EE heuristics)\\n\\t3. $Q(s,a) = Q(s,a) + \\\\alpha \\\\; \\\\delta_t$\\n\\t4. $a_{t+1}, s_{t+1} = a_t,s_t$\\n\\t', 'File: ../../notes/01-concepts/machine-learning/Natural Language/LLM & Thought.md\\n> [!error] Reasoning is not the same as the **classic definition of Reasoning**\\n## Chain of Thought\\n\\n## Tree of Thought\\n\\n## Graph of Thought\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "# Implement a simple cosine similarity function using numpy\n",
    "def cosine_similarity(a, b):\n",
    "  return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Implement a simple RAG function\n",
    "def retrival(question, k=5):\n",
    "  query_embedding = EMBEDDING_MODEL.encode(question)\n",
    "  similarities = cosine_similarity(VECTOR_DB_NP, query_embedding)\n",
    "  top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "  top_k_chunks = [VECTOR_DB[i][0] for i in top_k_indices]\n",
    "  return top_k_chunks\n",
    "\n",
    "# Test the RAG function\n",
    "top_k_chunks = retrival(question)\n",
    "print(top_k_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d2b512e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['File: ../../notes/01-concepts/machine-learning/Reinforcement/Exploration vs Exploitation.md\\nWhen Agents learn the practical problem is that the number of states and actions can grow indefinitely depending on the problem, and depending on the algorithm used, it might need more samples of the environment to learn, so the problem becomes, use a action already used or explore unknowns. \\n## Heuristics\\nTo try to mitigate this problem and bring some kind of balance there are several heuristics for saying if a agent should explore or should exploit. All of them can have some kind of decay, meaning it some how it can start decreasing the amount of exploration the agent does, this decay can be implemented in multiple forms like in each iteration decrease a value, or multiple by a value between 0 and 1 for faster decay. \\n### $\\\\epsilon$ greedy\\nThis is the most famous of them all. Which says that Agents should choose a random policy with probability $\\\\epsilon$ and should choose the best action with probability $1-\\\\epsilon$.\\n### Boltzmann\\nBased on the principal of [[Simulated Annealing]] it follows the Boltzmann equation of probability, and creates a probability for each state-action pair.\\n$$\\nP(s,a) =  \\\\frac{e^{Q(s,a) / T}}{\\\\sum_{b \\\\in A} e^{Q(s,b) / T}} \\n$$\\nIt obviously takes more time to compute then $\\\\epsilon$ Greedy.\\n\\n', 'File: ../../notes/01-concepts/machine-learning/Natural Language/Prompt Engineering.md\\nAre a set of techniques for creating better [[Language Models#Prompts|prompts]]. The main topic on it is context, there are many problems involved context, like missing information, bad training data, hallucination, ambiguity, etc.\\n\\n## Guidelines\\nThose are 5 main guidelines for creating better prompts.\\n\\n1. **Clarity:** It must be exact about what you want.\\n2. **Context**: More information about the subject helps the [[Language Models|LM]] to respond better, but they must be relevant, sufficient and not large as it can be to avoid [[Language Models#Prompts|Context Window]] problems. \\n3. **Objective:** Be concise and avoid ambiguity.\\n4. **Detail:** Detail makes guides better the response.\\n5. **Purpose:** What is the objective. \\n\\nOther useful stuff is to deal with [[Model Learning#Hyperparameters|Hyperparameters]] for instance lower temperature tend to avoid hallucination.\\n\\n\\n## Roles \\n\\n\\n\\n## Few, one and zero Shot\\n\"Shot\" in this sense is examples of response. \\n\\n## Iterative Prompting\\nImproving by testing the prompt. Its the most common and simple technique of prompt engineering that can be used together with other techniques. \\n\\n## Chain of thought (CoT) prompt\\nIs a induction technique to induce the response of the [[Language Models]] linearly, some of the most common:\\n* Asking for solving step by step\\n* Ask it to argument its response\\n* Giving fixed options for its response\\n\\n**Example:**\\n```\\nProblem: Sarah has 15 apples. She gives 1/3 of them to her friend and then buys 8 more. How many apples does she have now?\\n\\nSolve this step by step:\\n1. First, calculate how many apples Sarah gave away\\n2. Then, calculate how many apples she has left\\n3. Finally, add the new apples she bought\\n4. Provide the final answer\\n```\\n## Tree of thought (ToT) prompt\\nIt\\'s a hierarchic technique for prompting, this is some template for it:\\n\\n**Example:**\\n```\\nTask: Plan a weekend trip to a new city with a $500 budget.\\n\\nExplore different approaches:\\n\\nBranch 1: Focus on accommodation\\n- Option A: Luxury hotel ($300) → limited budget for activities\\n- Option B: Budget hostel ($80) → more budget for experiences\\n- Option C: Airbnb ($150) → balanced approach\\n\\nBranch 2: Focus on transportation\\n- Option A: Rent a car ($200) → freedom but high cost\\n- Option B: Public transport ($30) → economical but limited\\n- Option C: Bike rental ($50) → eco-friendly and flexible\\n\\nEvaluate each branch and select the best combination based on:\\n1. Total cost\\n2. Experience quality\\n3. Flexibility\\n\\nProvide the optimal plan.\\n```\\n\\n## Graph of thought (GoT)\\nExtends ToT by allowing thoughts to connect in a non-hierarchical graph structure, enabling cycles and multiple connections between concepts.\\n\\n**Example:**\\n```\\nTask: Debug why a web application is running slowly.\\n\\nCreate a graph of interconnected potential causes:\\n\\nNode A: Slow database queries\\n  → connects to Node C (inefficient indexes)\\n  → connects to Node E (too many connections)\\n\\nNode B: Large bundle size\\n  → connects to Node D (unoptimized images)\\n  → connects to Node F (unused dependencies)\\n\\nNode C: Inefficient indexes\\n  → connects back to Node A (affects query speed)\\n  → connects to Node G (poor database schema)\\n\\nNode D: Unoptimized images\\n  → connects to Node B (increases bundle)\\n  → connects to Node H (slow CDN delivery)\\n\\nNode E: Too many connections\\n  → connects to Node A (database bottleneck)\\n  → connects to Node I (connection pooling issues)\\n\\nAnalyze the graph to identify:\\n1. Most connected nodes (likely root causes)\\n2. Cycles that compound the problem\\n3. Priority order for fixes\\n```\\n\\n## Prompt Agentic\\nIts based on an autonomous agent approach for the LM, its principal are to plan, decide, execute action, evaluate the result and iterate over it. The prompt must define behavior of the agent, responsibility, limits, decision criteria, a \"reasoning\" and \"action\" loop.\\n\\nThis is one of the best resulting prompt techniques.\\n\\n**Example:**\\n```\\nYou are a research agent tasked with analyzing market trends for electric vehicles.\\n\\nBehavior: You are analytical, data-driven, and thorough.\\nResponsibility: Gather data, identify trends, and provide actionable insights.\\nLimits: Use only publicly available data from 2020-2025. Do not speculate beyond available data.\\n\\nDecision Criteria:\\n- Prioritize data from reputable sources (government reports, industry publications)\\n- Cross-reference multiple sources for validation\\n- Focus on trends with statistical significance\\n\\nReasoning and Action Loop:\\n1. PLAN: Identify what information is needed\\n2. ACT: Describe what data sources you would access\\n3. OBSERVE: Summarize the findings from each source\\n4. REASON: Analyze patterns and connections\\n5. DECIDE: Determine if more information is needed or if conclusions can be drawn\\n6. ITERATE: If needed, return to step 1 with refined focus\\n\\nTask: Analyze the adoption rate of electric vehicles in Europe from 2020-2025.\\nBegin with your plan.\\n``` ', 'File: ../../notes/01-concepts/machine-learning/Unsupervised/Principal Component Analyses.md\\n#ML \\n\\n', 'File: ../../notes/01-concepts/machine-learning/Reinforcement/Sarsa.md\\nIs a [[Markov Decision Process (MDP)#Learning a MDP|on policy]] algorithm based on [[Temporal Difference (TD) Learning|TD Control]], it uses the following variation:\\n\\n$$\\n\\\\begin{gather}\\n\\\\delta_t = r_{t+1} + \\\\gamma \\\\; Q(s_{t+1},a_{t+1}) - Q(s_t,a_t) \\\\\\\\ \\\\\\\\ \\nQ(s_t,a_t) = Q(s_t,a_t) + \\\\alpha \\\\; \\\\delta_t\\n\\\\end{gather}\\n$$\\n\\nThe overall algorithm does as following:\\n1. Start with a zeroed Q matrix.\\n2. Initialize current state and action using a [[Exploration vs Exploitation#Heuristics|EE heuristics]].\\n3. For each step:\\n\\t1. Take a observation  $a_t \\\\to (s_t,r_t)$\\n\\t2. Choose some action (using the EE heuristics)\\n\\t3. $Q(s,a) = Q(s,a) + \\\\alpha \\\\; \\\\delta_t$\\n\\t4. $a_{t+1}, s_{t+1} = a_t,s_t$\\n\\t', 'File: ../../notes/01-concepts/machine-learning/Natural Language/LLM & Thought.md\\n> [!error] Reasoning is not the same as the **classic definition of Reasoning**\\n## Chain of Thought\\n\\n## Tree of Thought\\n\\n## Graph of Thought\\n\\n']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Okay, I understand. Based on the provided context, here’s a summary of how agents handle the exploration vs. exploitation dilemma in reinforcement learning:\n",
       "\n",
       "**The Problem:** When dealing with complex environments (many states and actions), algorithms often need a lot of data to learn effectively. This creates a challenge: should the agent continue to explore new, potentially rewarding actions (exploration) or stick with actions it already knows are good (exploitation)?\n",
       "\n",
       "**Heuristics for Balance:** To address this, several heuristics are used to guide the balance between exploration and exploitation. These heuristics often involve a \"decay\" mechanism – a way to gradually reduce exploration as the agent learns.\n",
       "\n",
       "**Common Heuristics:**\n",
       "\n",
       "*   **Epsilon-Greedy:** This is the most common method. The agent randomly chooses an action with probability ε (epsilon) and chooses the best-known action with probability 1 - ε.\n",
       "*   **Boltzmann (or Softmax) Approach:** This method uses the Boltzmann distribution to assign probabilities to actions based on their Q-values (expected rewards). It relies on the Boltzmann equation:\n",
       "\n",
       "    `P(s,a) =  (e^(Q(s,a) / T)) / sum_b (e^(Q(s,b) / T))`\n",
       "\n",
       "    where:\n",
       "    *   `Q(s,a)` is the Q-value for state `s` and action `a`.\n",
       "    *   `T` is a temperature parameter (higher T = more exploration, lower T = more exploitation).\n",
       "\n",
       "**Algorithm: Sarsa**\n",
       "\n",
       "*   Sarsa is a specific \"on-policy\" algorithm based on Temporal Difference (TD) learning.\n",
       "*   It updates the Q-value based on the immediate reward and the estimated Q-value of the *next* state and action.\n",
       "*   The update rule is:\n",
       "\n",
       "    `delta_t = r_{t+1} + gamma * Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)`\n",
       "    `Q(s_t, a_t) = Q(s_t, a_t) + alpha * delta_t`\n",
       "\n",
       "    Where:\n",
       "    *   `alpha` is the learning rate.\n",
       "    *   `gamma` is the discount factor.\n",
       "\n",
       "**Important Note:**  The context also highlights that reasoning, as traditionally defined, is different from techniques like Chain of Thought, Tree of Thought, Graph of Thought, and Prompt Agentic.\n",
       "\n",
       "Do you have a specific question you'd like me to answer, or would you like me to elaborate on a particular aspect of this information?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def RAG(question, k=5):\n",
    "  top_k_chunks = retrival(question, k)\n",
    "  print(top_k_chunks)\n",
    "  k_str = '\\n'.join([f' - {chunk}' for chunk in top_k_chunks])\n",
    "  instruction_prompt = f'''You are a helpful chatbot.\n",
    "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
    "{k_str}\n",
    "'''\n",
    "  response = ollama.generate(model=LM_VERSION, prompt=instruction_prompt)\n",
    "  return response\n",
    "\n",
    "response = RAG(question)\n",
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0340e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let's break down two key types of Enterprise Experience (EE) heuristics. These heuristics, developed by McKinsey, help organizations understand and address the complexities of transforming customer experiences across all touchpoints.\n",
       "\n",
       "**1. The \"5 Pillars\" Heuristics:**\n",
       "\n",
       "This is arguably the most well-known and foundational set of EE heuristics. It identifies five key areas that need to be addressed for successful EE transformation.\n",
       "\n",
       "*   **Customer Journey:**  This pillar focuses on understanding the *actual* customer journey, not just the intended one. It involves mapping, analyzing, and optimizing the steps customers take when interacting with your brand, from initial awareness to post-purchase support. It's about uncovering pain points, moments of truth, and opportunities for improvement.\n",
       "*   **Channels:**  This pillar looks at how customers interact with your brand across all available channels – website, mobile app, email, social media, physical store, call center, etc.  It’s not just about *having* channels, but about how they’re connected, integrated, and working *together* for a seamless experience.  Duplication and silos are major concerns.\n",
       "*   **Digital Integration:**  This is about unifying digital experiences. It addresses how digital channels (like websites and apps) connect with offline channels (like stores and customer service). It's about creating a consistent and personalized experience regardless of where the customer interacts.\n",
       "*   **Data & Analytics:**  This pillar highlights the critical role of data in driving EE.  It's about capturing, analyzing, and using customer data to understand behaviors, personalize experiences, and measure the impact of changes.  It’s not just about collecting data, but turning it into actionable insights.\n",
       "*   **Organization & Culture:** This focuses on the people and processes within the organization. It recognizes that EE success requires a cultural shift – one where customer experience is truly embedded in everyone's thinking and actions, not just the responsibility of a single department. It includes things like empowered teams, cross-functional collaboration, and a customer-centric mindset.\n",
       "\n",
       "\n",
       "\n",
       "**2. The \"Four Dimensions\" Heuristics:**\n",
       "\n",
       "This heuristic focuses on the depth of the EE effort. It’s a more granular framework that builds upon the 5 pillars by organizing them into four distinct dimensions.\n",
       "\n",
       "*   **Depth:** This dimension concerns *how deeply* you're exploring the customer experience.  It ranges from shallow (e.g., simply redesigning a website) to deep (e.g., fundamentally rethinking your business model around the customer).  This is about the level of analysis and the scope of changes.\n",
       "*   **Breadth:** This dimension refers to *how many* customer journeys and touchpoints you are addressing. Are you focused on a core customer segment or attempting to optimize the entire customer experience?\n",
       "*   **Speed:** This dimension assesses *how quickly* you're implementing changes. Rapid experimentation and iterative improvements are crucial for success. It's about balancing thoroughness with agility.\n",
       "*   **Scope:** This dimension looks at *what* you are focusing on. Is it a specific channel, product, or customer segment?\n",
       "\n",
       "\n",
       "\n",
       "**Key Differences & Relationship:**\n",
       "\n",
       "*   The \"5 Pillars\" provide a broad strategic framework for EE initiatives.\n",
       "*   The \"Four Dimensions\" provide a more tactical framework for planning and executing those initiatives, by breaking them down into measurable aspects.\n",
       "*   You'd typically use the 5 pillars to *define* the strategic goals for your EE transformation, and then use the 4 dimensions to *plan* how to achieve those goals.\n",
       "\n",
       "---\n",
       "\n",
       "**Resources for Further Learning:**\n",
       "\n",
       "*   **McKinsey & Company – Enterprise Experience:** [https://www.mckinsey.com/industries/retail/our-insights/enterprise-experience](https://www.mckinsey.com/industries/retail/our-insights/enterprise-experience) (This is the primary source for these heuristics).\n",
       "*   **Harvard Business Review – Enterprise Experience:** [https://hbr.org/topic/enterprise-experience](https://hbr.org/topic/enterprise-experience)\n",
       "\n",
       "Do you want me to delve deeper into a specific aspect of these heuristics, such as:\n",
       "\n",
       "*   A particular pillar in more detail?\n",
       "*   How to apply these heuristics in a specific industry?\n",
       "*   How these heuristics compare to other customer experience frameworks?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# without rag\n",
    "response = ollama.generate(model=LM_VERSION, prompt=question)\n",
    "display(Markdown(response.response))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
