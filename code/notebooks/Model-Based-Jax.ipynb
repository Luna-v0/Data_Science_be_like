{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4817c11a",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "In this notebook the simple Frozen Lake environment of Gymnasium will be study. But using JAX\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec674b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a061629",
   "metadata": {},
   "source": [
    "## Setting up the Environments\n",
    "I created one for training to go faster and another for visualising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36460952",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_env = gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=True)\n",
    "testing_env = gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=True, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e673b",
   "metadata": {},
   "source": [
    "This are the states and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = training_env.observation_space\n",
    "actions = training_env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f2c73",
   "metadata": {},
   "source": [
    "## Collection the Data Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9786f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS = 500000\n",
    "current_state, _ = training_env.reset()\n",
    "num_actions = actions.n\n",
    "num_states = states.n\n",
    "gamma = 0.99\n",
    "\n",
    "data_points = jnp.zeros((STEPS, 4))\n",
    "\n",
    "\n",
    "for i in range(STEPS):\n",
    "    action = np.random.choice(range(num_actions))\n",
    "    next_state, reward, terminated, truncated, info = training_env.step(action)\n",
    "    data_points.at[i].set(jnp.array([current_state, action, reward, next_state]))\n",
    "    current_state = next_state\n",
    "    if terminated or truncated:\n",
    "        current_state, info = training_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e9212",
   "metadata": {},
   "source": [
    "## Building the Reward Matrix and Transition Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f48d03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_tensor = jnp.zeros((states.n, actions.n, states.n), dtype=np.float32)\n",
    "\n",
    "state_action_counter = jnp.zeros((states.n, actions.n), dtype=jnp.int32)\n",
    "# for each action states x states\n",
    "state_action_tensor = jnp.zeros((states.n, actions.n,states.n), dtype=jnp.float32)\n",
    "\n",
    "reward_matrix = jnp.zeros((states.n, actions.n), dtype=jnp.float32)\n",
    "\n",
    "for i in range(STEPS):\n",
    "    current_state, action, reward, next_state = data_points[i]\n",
    "    current_state = int(current_state)\n",
    "    action = int(action)\n",
    "    next_state = int(next_state)\n",
    "    state_action_counter[current_state, action] += 1\n",
    "    state_action_tensor[current_state, action, next_state] += 1\n",
    "    reward_matrix[current_state, action] += reward\n",
    "\n",
    "# Normalize transition_tensor over next states (axis=2), not over actions!\n",
    "# transition_tensor[state, action, next_state] = P(next_state | state, action)\n",
    "transition_tensor = state_action_tensor / jnp.sum(state_action_tensor, axis=2, keepdims=True)\n",
    "# Normalize reward_matrix by the count of (state, action) visits\n",
    "reward_matrix = reward_matrix / jnp.maximum(state_action_counter, 1)  # Avoid division by zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d8b8a9",
   "metadata": {},
   "source": [
    "Just filtering some NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f860f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn all the nans into 0\n",
    "reward_matrix = jnp.nan_to_num(reward_matrix, -1)\n",
    "\n",
    "transition_tensor = jnp.nan_to_num(transition_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6813241d",
   "metadata": {},
   "source": [
    "## The core Value Iteration algorithm\n",
    "Here a simple [[Iteration Algorithms#Value Iteration|Value Iteration]] was implemented, which can be further optimised to build the transition tensor, and reward through Value Iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240da820",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = 1\n",
    "i = 0\n",
    "\n",
    "V = np.ones(states.n)\n",
    "\n",
    "\n",
    "\n",
    "while err > 1e-8:\n",
    "  Q = np.zeros((states.n, num_actions))\n",
    "\n",
    "  for action in range(num_actions):\n",
    "    Q[:,action] = reward_matrix[:,action] + gamma * transition_tensor[:,action] @ V\n",
    "\n",
    "  Vnew = np.max(Q, axis=1)\n",
    "  err = np.linalg.norm(V - Vnew)\n",
    "  V = Vnew\n",
    "  i+=1\n",
    "\n",
    "np.argmax(Q, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2264fcdd",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "obs, info = testing_env.reset()\n",
    "\n",
    "while True:\n",
    "    action = np.argmax(Q[obs])\n",
    "    obs, reward, terminated, truncated, info = testing_env.step(action)\n",
    "    # sleep for 0.1 seconds\n",
    "    time.sleep(0.1)\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc47522d",
   "metadata": {},
   "source": [
    "Note that sometimes it will fall into the whole since it has a probability of falling on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e346b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_env.close()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
